<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Fundamentos de inferencia - 1&nbsp; Distribución en el muestreo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Distribución en el muestreo</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Fundamentos de inferencia</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Prólogo</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-distribucion_en_el_muestreo.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Distribución en el muestreo</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">Referencias</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción"><span class="toc-section-number">1.1</span>  Introducción</a>
  <ul class="collapse">
  <li><a href="#ejemplos" id="toc-ejemplos" class="nav-link" data-scroll-target="#ejemplos">Ejemplos</a></li>
  </ul></li>
  <li><a href="#muestra-aleatoria" id="toc-muestra-aleatoria" class="nav-link" data-scroll-target="#muestra-aleatoria"><span class="toc-section-number">1.2</span>  Muestra aleatoria</a></li>
  <li><a href="#media-de-las-variables-de-una-muestra." id="toc-media-de-las-variables-de-una-muestra." class="nav-link" data-scroll-target="#media-de-las-variables-de-una-muestra."><span class="toc-section-number">1.3</span>  Media de las variables de una muestra.</a></li>
  <li><a href="#varianza-muestral" id="toc-varianza-muestral" class="nav-link" data-scroll-target="#varianza-muestral"><span class="toc-section-number">1.4</span>  Varianza muestral</a></li>
  <li><a href="#distribución-de-la-media-y-de-la-varianza-muestrales-para-una-m.a.s-de-una-distribución-normal." id="toc-distribución-de-la-media-y-de-la-varianza-muestrales-para-una-m.a.s-de-una-distribución-normal." class="nav-link" data-scroll-target="#distribución-de-la-media-y-de-la-varianza-muestrales-para-una-m.a.s-de-una-distribución-normal."><span class="toc-section-number">1.5</span>  Distribución de la media y de la varianza muestrales para una m.a.s de una distribución normal.</a></li>
  <li><a href="#distribución-t-de-student" id="toc-distribución-t-de-student" class="nav-link" data-scroll-target="#distribución-t-de-student"><span class="toc-section-number">1.6</span>  Distribución t de Student</a></li>
  <li><a href="#distribución-f-de-snedecor-para-el-cociente-de-varianzas." id="toc-distribución-f-de-snedecor-para-el-cociente-de-varianzas." class="nav-link" data-scroll-target="#distribución-f-de-snedecor-para-el-cociente-de-varianzas."><span class="toc-section-number">1.7</span>  Distribución F de Snedecor para el cociente de varianzas.</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Distribución en el muestreo</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introducción" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="introducción"><span class="header-section-number">1.1</span> Introducción</h2>
<p>Estamos interesados en modelizar un fenómeno que presenta un aspecto aleatorio. Para plantear y contestar preguntas sobre probabilidades de sucesos asociados, buscamos disponer de un modelo para la distribución de los valores de la variable de interés. Iremos construyendo nuestro conocimiento sobre esta distribución al recopilar una muestra de valores, que iremos recogiendo al repetir un experimento.</p>
<section id="ejemplos" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ejemplos">Ejemplos</h3>
<ul>
<li>Disponemos de una moneda para tirar a cara o cruz, pero queremos asegurarme de que no está trucada. Para contestar a esta pregunta, planteamos el modelo siguiente: si la variable <span class="math inline">\(X\)</span> recoge el resultado de experimento que consiste en tirar una vez la moneda, puede tomar los valores <span class="math inline">\(c\)</span> (Cara) o <span class="math inline">\(+\)</span> (Cruz) con las probabilidades: <span class="math inline">\(\mathbb{P}[X=c]=p\)</span> y <span class="math inline">\(\mathbb{P}[X=+]=1-p.\)</span> La cantidad <span class="math inline">\(p\)</span> es por lo tanto la probabilidad de que salga cara, y es un parámetro de nuestro modelo. En el caso en que confiamos en que la moneda no está trucada, el parámetro <span class="math inline">\(p\)</span> tomará el valor <span class="math inline">\(1/2\)</span>. Para sacar información sobre <span class="math inline">\(p\)</span> y comprobar en particular que la moneda no está trucada, repetiremos un cierto número de veces el experimento.</li>
<li>Para las próximas elecciones generales, queremos determinar la proporción de gente que tiene intención de ir a votar, es decir queremos estimar la tasa de participación. El censo electoral para España tiene unos 35 millones de personas. Es claramente imposible entrevistar a todas las personas del censo, pero puedo realizar una encuesta para conseguir una muestras de respuestas. Planteamos por lo tanto un modelo simple correspondiente al experimento de escoger al azar a una persona censada: si <span class="math inline">\(X\)</span> es la respuesta a la pregunta “Tiene intención de ir a votar?”, puede tomar dos valores 1 ó 0 que codifican las respuestas “Sí” o “No” respectivamente. Llamamos <span class="math inline">\(p\)</span> la tasa de intención de voto, es decir que <span class="math inline">\(\mathbb{P}(X = 1) = p\)</span>. Quieremos estimar <span class="math inline">\(p\)</span>, y lo haremos repetiendo el experimento de escoger una persona al azar en el censo y preguntarle si tiene intención de ir a votar.</li>
<li>El índice de audiencias manda en la programación de televisión. Pero ¿cómo saben cuántos espectadores vieron un partido dado o un programa determinado? Claramente, no preguntan a todos los potenciales espectadores. En realidad, una encuesta se realiza de manera automática y continua: una empresa especializada llamada Kantar media, <a href="https://www.kantar.com/es">enlace a su página web</a>, ha escogido al azar unos 6000 hogares que representan unas 20000 personas de entre un total de más de 40 000 000 espectadores potenciales. En cada uno de estos hogares, instala un aparato llamado ``audímetro’’ que registra cuál es el programa que se está viendo en cada momento.</li>
<li>Queremos conocer la concentración de un determinado producto en una solución. Pensamos que un modelo razonable para la distribución de los valores proporcionados por nuestro aparato de medición sea una distribución normal con media <span class="math inline">\(\mu\)</span> y desviación típica <span class="math inline">\(\sigma\)</span> desconocidas. El centro de esta distribución, es decir <span class="math inline">\(\mu\)</span>, será por lo tanto lo más representativo de la concentración que intento determinar. Para estimar <span class="math inline">\(\mu\)</span>, repetiremos la medición varias veces.</li>
</ul>
<p>Pero surge una pregunta evidente:</p>
<p><strong>Pregunta</strong> ¿Cómo sabemos que nuestra estimación es fiable? ¿Por qué limitándose a unas 20000 personas, se puede extropolar el resultado con confianza a una población de decenas de millones? Además está claro que el resultado que obtengo depende de la muestra particular que haya escogido, si hubiera escogido otra muestra, habría obtenido otro resultado. Este hecho se llama la variabilidad muestral.</p>
<p>Para contestar a esta pregunta debemos procurar estudiar la variabilidad muestral, es decir conocer la distribución de los valores que toma una cantidad calculada a partir de una muestra concreta, como la proporción por ejemplo, respecto a todas las muestras que podría extraer. Si esta distribución presenta poca dispersión, tendremos bastante confianza en que el proceso de estimación es poco sensible a la muestra concreta escogida, puesto que la probabilidad de que otra muestra haya arrojado un valor muy distinto será pequeña.</p>
<p>Una manera de realizar este estudio de la variabilidad muestral es a través de simulaciones realizadas con el ordenador. Podemos simular la extracción repetida de muestras de tamaño 20000 de una población. Supongamos por ejemplo, que consideramos la retransmisión en España de la final de la copa del Mundo 2022 que consiguió una cuota de casí 70%, entre los 17000000 espectadores en ese momento, fuente <a href="https://www.rtve.es/rtve/20221219/audiencias-gran-final-mundial-catar/2412248.shtml">noticia en rtve.es</a>. Supongamos que la proporción de espectadores que vieron la final fue realmente 0. 7. Vamos a definir un vector con 17000000 elementos de los cuales el 70%, es decir 11900000, son “1”s, y el 30%, es decir 5100000, son “0”s. Este vector representará la “población” de todos los espectadores en este momento. Los “1”s representan las personas que vieron la final por la tele. Escogeremos al azar una muestra de 20000 elementos entre los elementos del vector población y calcularemos la proporción de “1”s en esta muestra. Comprobaremos si está cerca de la proporción de “1”s en la población, que vale 0.7.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>poblacion <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">11900000</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">5100000</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">314159</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>p_muestra <span class="ot">=</span>  <span class="fu">mean</span>(<span class="fu">sample</span>(poblacion, <span class="dv">20000</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>p_muestra</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7</code></pre>
</div>
</div>
<p>Para esta muestra concreta, la proporción muestral de “1”s es muy próxima a la proporción poblacional, por lo que nuestra estimación usando una muestra de “solo” 20000 personas nos da muy buen resultado.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Nota
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>En el trozo de código anterior, hemos usado la función <code>set.seed</code> que permite fijar la semilla de la secuencia de números pseudoaleatorios que se van a generar. Admite un número como argumento, que podemos escoger como una cantidad fácil de recordar. El fijar la semilla permite reproducir simulaciones aunque impliquen la generación de números aleatorios.</li>
<li>Para calcular la proporción muestral de “1”s, hemos usado la media muestral, puesto que la suma de los valores en la muestra es igual al número de “1”s que presenta.</li>
</ul>
</div>
</div>
<p>Para convencernos de que este buen comportamiento es lo esperable para muestras de este tamaño, vamos a repetir muchas veces (10000 veces por ejemplo), la extracción de una muestra de 20000 personas en la población. Para hacerlo, podríamos repetir el código anterior en un bucle <code>for</code>, pero vamos a aprovechar la librería <code>purrr</code> del “tidyverse”, que permite aplicar una función a los elementos de una lista o de un vector.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(purrr)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>p_muestras <span class="ot">=</span> <span class="fu">map_dbl</span>(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span><span class="sc">:</span><span class="dv">10000</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span> <span class="fu">mean</span>(<span class="fu">sample</span>(poblacion, <span class="dv">20000</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(p_muestras)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.70430 0.69660 0.69565 0.70330 0.70065 0.69795</code></pre>
</div>
</div>
<p>Podemos ahora representar un histograma de las 10000 proporciones muestrales obtenidas y calcular algún percentil asociado.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    p_muestras,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">"Proporción muestral"</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">"Frecuencia"</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">""</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlim =</span> <span class="fu">c</span>(<span class="fl">0.6</span>, <span class="fl">0.8</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-histograma-phat" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="01-distribucion_en_el_muestreo_files/figure-html/fig-histograma-phat-1.png" class="img-fluid figure-img" width="960"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;1.1: Histograma de los valores de la proporción muestral para las 10000 muestras extraidas.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Comprobamos en el histograma que todas las muestras simuladas llevan a una estimación de la proporción muy próxima a la proporción poblacional. De hecho, 99.9% de las muestras presentan un error menor de 1 punto respecto al valor poblacional y 87.4% de las muestras un error menor que medio punto.</p>
<p>En conclusión, este estudio de simulación nos lleva a tener confianza en la precisión de nuestra estimación si extraemos una muestra de 20000 personas para aproximar la proporción en una población de 17000000 personas. En este tema exploraremos esta misma idea de estudiar la “distribución en el muestreo” no solamente a través de simulaciones sino gracias a la obtención de resultados teóricos manipulando probabilidades.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Inferir
</div>
</div>
<div class="callout-body-container callout-body">
<p>Inferir sobre un parámetro presente en el modelo de distribución de una variable consiste precisamente en sacar información sobre su valor a partir de una muestra de valores de la variable.</p>
<p>Toda la teoría desarrollada acerca de los sondeos, del control de calidad, del diseño de experimentos, y en realidad, de toda la teoría estadistica se basa de manera esencial en el estudio de la variabilidad muestral de cantidades calculadas a partir de una muestra.</p>
</div>
</div>
</section>
</section>
<section id="muestra-aleatoria" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="muestra-aleatoria"><span class="header-section-number">1.2</span> Muestra aleatoria</h2>
<p>Como primer paso en el estudio de la distribución en el muestreo, formalizamos el concepto de muestra aleatoria simple como la variable multidimensional asociada a la repetición de un experimento simple.</p>
<div id="def-mas" class="theorem definition">
<p><span class="theorem-title"><strong>Definición 1.1 </strong></span>Sea una distribución <span class="math inline">\(f\)</span>. Consideramos <span class="math inline">\(n\)</span> variables aleatorias independientes <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, que tengan cada una la misma distribución <span class="math inline">\(f\)</span>. La variable aleatoria multidimensional <span class="math inline">\((X_1, X_2, \ldots, X_n)\)</span> es una muestra aleatoria simple de <span class="math inline">\(f\)</span>.</p>
</div>
<p>Decimos que las variables <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> son independientes e identícamente distribuidas, lo que se abrevia como i.i.d.</p>
<p><em>Nota:</em> Por la propiedad de independencia, si <span class="math inline">\(f\)</span> es una función de densidad y <span class="math inline">\((X_1, X_2, \ldots, X_n)\)</span> es una muestra aleatoria simple de <span class="math inline">\(f\)</span>, la función de densidad conjunta de la muestra es el producto de las marginales: <span class="math display">\[f_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n) = f(x_1)\cdot f(x_2)\cdot \cdots \cdot f(x_n).\]</span></p>
<p>La situación de modelización más habitual donde consideraremos una muestra aleatoria simple corresponde a la repetición <span class="math inline">\(n\)</span> veces de manera independiente de un experimento simple al que está asociado una variable <span class="math inline">\(X\)</span> con distribución <span class="math inline">\(f\)</span>. Si llamamos <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> a las variables que recogen los valores de <span class="math inline">\(X\)</span> obtenidos en las <span class="math inline">\(n\)</span> repeticiones, <span class="math inline">\((X_1, X_2, \ldots, X_n)\)</span> es una muestra aleatoria simple de <span class="math inline">\(f\)</span>.</p>
<p>Es el caso del ejemplo descrito en la sección anterior que consiste en tirar <span class="math inline">\(n\)</span> veces una moneda. Si <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> denotan los valores obtenidos (+ ó c), <span class="math inline">\((X_1, X_2, \ldots, X_n)\)</span> es una muestra aleatoria simple de <span class="math inline">\(f\)</span>, la distribución de <span class="math inline">\(X\)</span> definida como <span class="math inline">\(\mathbb{P}[X=c]=p\)</span> y <span class="math inline">\(\mathbb{P}[X=+]=1-p.\)</span></p>
<p>En cambio, si consideramos el ejemplo de una encuesta para estimar la tasa de participación, la población de la que se escoge la muestra es finita (unos 35 000 000), y se trata de una extracción sin reemplazo. La probabilidad de que la persona escogida conteste “Sí” va cambiando a medida que vamos entrevistando a gente y depende de las respuestas previas de los encuestados. Por lo tanto, las variables <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> no son i.i.d. y el vector asociado no forma una muestra aleatoria simple.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Para saber más
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Si <span class="math inline">\(N_S\)</span> es el número de personas en el censo que tienen intención de ir a votar, y <span class="math inline">\(N\)</span> es el tamaño del censo. Suponiendo que <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> toman valores 0 ó 1, con la convención de que <span class="math inline">\(x_i = 1\)</span> si el entrevistado número <span class="math inline">\(i\)</span> declara que tiene intención de ir a votar. Tenemos que <span class="math display">\[\mathbb{P}(X_i = 1) = \frac {N_S - \sum_{1\leq j&lt; i}x_j} {N - (i - 1)}.\]</span></p>
</div>
</div>
</div>
</section>
<section id="media-de-las-variables-de-una-muestra." class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="media-de-las-variables-de-una-muestra."><span class="header-section-number">1.3</span> Media de las variables de una muestra.</h2>
<p>Sin duda, entre las cantidades más relevantes asociadas a una muestra destaca la media muestral. Empezaremos por lo tanto por estudiar qué podemos decir sobre su distribución muestral, es decir la distribución de los valores que puede tomar respecto a todas las muestras que se podrían extraer. Como primer paso, la obtención de su esperanza y varianza es sencilla y nos proporciona información valiosa sobre su centro y su dispersión:</p>
<div id="prp-espvarxbar" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposición 1.1 </strong></span>Si <span class="math inline">\((X_1, X_2, \ldots, X_n)\)</span> es una muestra aleatoria simple de <span class="math inline">\(f\)</span>, que tiene esperanza y varianza, consideramos su media <span class="math inline">\(\bar{X}_n =\frac 1 n \sum_{i = 1, \ldots, n} X_i\)</span>, se cumple:</p>
<ul>
<li>Si <span class="math inline">\(\mu_f\)</span> es la esperanza de la distribución <span class="math inline">\(f\)</span>, <span class="math display">\[\mathbb{E}(\bar{X}_n)=  \mu_f.\]</span></li>
<li>Si <span class="math inline">\(\sigma_f^2\)</span> es la varianza de la distribución <span class="math inline">\(f\)</span>, <span class="math display">\[Var(\bar{X}_n)=  \sigma_f^2/n.\]</span></li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Prueba</em>. </span>La esperanza de <span class="math inline">\(\bar{X}\)</span> se obtiene usando la propiedad de linealidad de la esperanza, mientras que para su varianza, se usa el hecho de que la varianza de la suma de variables independientes es la suma de sus varianzas.</p>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Nota
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Es muy destacable que esos resultados son ciertos sea cual sea la distribución <span class="math inline">\(f\)</span>: el centro de los valores de la media muestral coincide con el centro de los valores de la distribución <span class="math inline">\(f\)</span> de la cual se muestrea, mientras que su dispersión, medida por su desviación típica, es raiz cuadrada de <span class="math inline">\(n\)</span> más pequeña que la dispersión en la población.</li>
<li>Si consideramos el experimento de realizar la medición de una cantidad usando un aparato, la variable <span class="math inline">\(X\)</span> denota el valor proporcionado por el aparato, su distribución es <span class="math inline">\(f\)</span>. Si el aparato es exacto, el centro de los valores de <span class="math inline">\(f\)</span> coincide con la cantidad que perseguimos determinar. La medición siempre está acompañada de un error, y este error es aleatorio, pero si el aparato está bien calibrado, <span class="math inline">\(\mu_f\)</span> coincide con la cantidad exacta. Si el aparato es preciso, presenta una buena reproducibilidad de las mediciones, es decir que <span class="math inline">\(\sigma_f\)</span> es pequeña. Se puede encontrar en la figura <a href="#fig-diana">Figura&nbsp;<span>1.2</span></a> una analogía de la medición con el disparo a una diana. Por la <a href="#prp-espvarxbar">Proposición&nbsp;<span>1.1</span></a>, deducimos que, si repetimos la medición varias veces y calculamos la media de los valores obtenidos, conseguimos mejorar la precisión (disminuir la dispersión) de nuestro procedimiento.</li>
</ul>
</div>
</div>
<div id="fig-diana" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/dianas.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;1.2: Analogía de la medición con una diana</figcaption><p></p>
</figure>
</div>
</section>
<section id="varianza-muestral" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="varianza-muestral"><span class="header-section-number">1.4</span> Varianza muestral</h2>
<p>Como medida de la dispersión de una muestra, es útil considerar la varianza muestral que definimos a continuación:</p>
<div id="def-varianzamuestral" class="theorem definition">
<p><span class="theorem-title"><strong>Definición 1.2 </strong></span>Si <span class="math inline">\((X_1, X_2, \ldots, X_n)\)</span> es una muestra aleatoria simple de <span class="math inline">\(f\)</span>, definimos la varianza muestral <span class="math inline">\(S_n^2\)</span> como <span class="math display">\[ S_n^2 = \frac 1 {n - 1} \sum_{i = 1}^n \left(X_i - \bar{X}_n\right)^2.\]</span> Es sencillo demostrar la fórmula alternativa para <span class="math inline">\(S_n^2\)</span>: <span class="math display">\[S_n^2 = \frac n {n - 1} \left(\bar{X^2}_n - (\bar{X}_n)^2\right),\]</span> donde <span class="math inline">\(\overline{X^2}_n= \frac 1 n \sum_{1\leq i \leq n} X_i^2\)</span> es la media de los cuadrados de los valores de la muestra.</p>
</div>
<p>Antes de obtener la distribución de la varianza muestral en el caso de una distribución normal, podemos obtener su esperanza, para cualquier distribución <span class="math inline">\(f\)</span> con varianza finita.</p>
<div id="prp-espvar" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposición 1.2 </strong></span>Si <span class="math inline">\((X_1, X_2, \ldots, X_n)\)</span> es una muestra aleatoria simple de <span class="math inline">\(f\)</span> con varianza <span class="math inline">\(\sigma_f^2\)</span>, <span class="math display">\[\mathbb{E}[S_n^2] = \sigma_f^2.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Prueba</em>. </span>Escribimos <span class="math display">\[\mathbb{E}[S_n^2] =\frac 1 {n - 1} \sum_{i = 1}^n \mathbb{E}\left[(X_i - \bar{X}_n)^2\right]= \frac 1 {n-1}\sum_{i = 1}^n \mathbb{E}\left[\left(X_i -\mu_f - \frac 1 n\sum_{1\leq j\leq n}(X_j -\mu_f)\right)^2\right].\]</span> Ahora, <span class="math display">\[\left(X_i -\mu_f - \frac 1 n\sum_{1\leq j\leq n}(X_j -\mu_f)\right)^2 = \left(\frac {n - 1} n (X_i - \mu_f) - \frac 1 n\sum_{1\leq j\leq n, j\neq i}(X_j -\mu_f)\right)^2\]</span> Al desarrollar este cuadrado nos encontramos con <span class="math inline">\((\frac {n - 1} n (X_i - \mu_f))^2 + \frac 1 n^2\sum_{1\leq j\leq n, j\neq i}(X_j -\mu_f)^2\)</span> y con productos de la forma <span class="math inline">\((\frac {n - 1} n (X_i - \mu_f)\frac 1 n(X_j -\mu_f))\)</span>, para <span class="math inline">\(j\neq i\)</span>. Todos estos últimos productos tienen esperanza nula porque las variables <span class="math inline">\(X_i\)</span> y <span class="math inline">\(X_j\)</span> son independientes.</p>
<p>Obtenemos en consecuencia que <span class="math display">\[\begin{align}
\mathbb{E}[S_n^2] &amp;=&amp; \frac 1 {n - 1} \sum_{i = 1}^n \mathbb{E}\left[(\frac {n - 1} n (X_i - \mu_f))^2+ \frac 1 {n^2}\sum_{1\leq j\leq n, j\neq i}(X_j -\mu_f)^2\right]\\
&amp;=&amp; \frac 1 {n - 1} n\left(\frac {(n -1)^2}{n^2}+ \frac {n - 1} {n^2}\right)\sigma_f^2\\
&amp;=&amp; \sigma_f^2.
\end{align}\]</span></p>
</div>
<p>Deducimos de la <a href="#prp-espvar">Proposición&nbsp;<span>1.2</span></a> que el centro de los valores de la distribución de la varianza muestral coincide con la varianza de <span class="math inline">\(f\)</span>, que llamamos también la varianza “poblacional”.</p>
</section>
<section id="distribución-de-la-media-y-de-la-varianza-muestrales-para-una-m.a.s-de-una-distribución-normal." class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="distribución-de-la-media-y-de-la-varianza-muestrales-para-una-m.a.s-de-una-distribución-normal."><span class="header-section-number">1.5</span> Distribución de la media y de la varianza muestrales para una m.a.s de una distribución normal.</h2>
<p>En las secciones anteriores, hemos caracterizado la esperanza y la varianza de la distribución de los valores de la media muestral <span class="math inline">\(\bar{X}_n\)</span> así como la esperanza de la varianza muestral <span class="math inline">\(S^2_n\)</span>. Es destacable el hecho de que estos resultados se obtienen sin hipótesis sobre la forma de la distribución <span class="math inline">\(f\)</span> de <span class="math inline">\(X\)</span>. ¿Podemos decir algo más sobre la distribución de estos estadísticos si asumimos un modelo concreto para <span class="math inline">\(f\)</span>? En el caso en que hemos modelizado la v.a <span class="math inline">\(X\)</span> por una distribución Normal <span class="math inline">\(\mathcal{N} (\mu,\sigma^2)\)</span> somos capaces de caracterizar con detalles la distribución muestral de <span class="math inline">\(\bar{X}_n\)</span> y de <span class="math inline">\(S^2_n\)</span>, e incluso obtener un resultado sobre su distribución conjunta. La siguiente proposición establece resultados de gran importancia para la teoría de la inferencia.</p>
<div id="prp-distxbar" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposición 1.3 </strong></span>Consideramos una muestra aleatoria simple de una distribución normal <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>, <span class="math inline">\(\bar{X}_n\)</span> es su media muestral y <span class="math inline">\(S^2_n\)</span> la varianza muestral, se cumple</p>
<ol type="1">
<li><span class="math inline">\(\bar{X}_n\)</span> y <span class="math inline">\(S^2_n\)</span> son dos v.a. independientes.</li>
<li><span class="math display">\[\bar{X}_n\sim \mathcal{N}(\mu,\frac {\sigma^2} n),\]</span> o, de manera equivalente, <span id="eq-xbarestandar"><span class="math display">\[\frac{\bar{X}_n-\mu} {\sigma/\sqrt{n}} \sim \mathcal{N}(0,1). \tag{1.1}\]</span></span></li>
<li>La variable <span class="math inline">\((n - 1)S_n^2/\sigma^2\)</span> sigue una distribución ji-cuadrado con <span class="math inline">\(n - 1\)</span> grados de libertad, que denotamos por <span class="math inline">\((n - 1)S_n^2/ \sigma^2\sim \chi^2_{n-1}\)</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Prueba</em>. </span>Cualquier combinación de variables normales sigue una distribución normal, por lo que deducimos que <span class="math inline">\(\bar{X}_n\)</span> tiene una distribución normal, con esperanza y varianza obtenidas en la <a href="#prp-espvar">Proposición&nbsp;<span>1.2</span></a>, lo que demuestra el punto 2.</p>
<p>Por otra parte, calculemos la covarianza de <span class="math inline">\(\bar{X}_n\)</span> con cualquier <span class="math inline">\(X_i - \bar{X}_n\)</span>, para <span class="math inline">\(i = 1,\ldots, n\)</span>: <span class="math display">\[\begin{align*}
\mathbb{E}[(X_i-\bar{X}_n)(\bar{X}_n - \mu)] &amp;=&amp; \mathbb{E}[(X_i-\mu + \mu - \bar{X}_n)(\bar{X}_n - \mu)]\\
&amp;=&amp; \mathbb{E}[(X_i - \mu)(\bar{X}_n - \mu)] -\mathbb{E}[ (\bar{X}_n - \mu)^2]\\
&amp;=&amp; \frac 1 n \sum_{j=1}^n\mathbb{E}[(X_i - \mu)(X_j - \mu)] - \sigma^2/n\\
\end{align*}\]</span> Al ser las variables <span class="math inline">\(X_1, \ldots, X_n\)</span> independientes, <span class="math inline">\(\mathbb{E}[(X_i - \mu)(X_j - \mu)] = 0\)</span> excepto cuando <span class="math inline">\(i=j\)</span>, en cuyo caso, es igual a la varianza de <span class="math inline">\(X_i\)</span>, es decir <span class="math inline">\(\sigma^2\)</span>. Concluimos que <span class="math inline">\(cov(\bar{X}_n, X_i - \bar{X}_n) = 0\)</span> para todo <span class="math inline">\(i = 1, \ldots, n\)</span>. Puesto que todas estas variables son variables normales, deducimos que <span class="math inline">\(\bar{X}_n\)</span> es independiente de <span class="math inline">\(X_i - \bar{X}_n\)</span> para todo <span class="math inline">\(i = 1, \ldots, n\)</span> y, por lo tanto, independiente de cualquier función de <span class="math inline">\(X_1 - \bar{X}_n, \ldots, X_n - \bar{X}_n\)</span>, en particular de <span class="math inline">\(S_n^2\)</span>. Esto demuestra el punto 1.</p>
<p>Para demostrar el punto 3., procederemos por inducción sobre <span class="math inline">\(n\)</span>. Recordemos que el cuadrado de una variable normal estándar es una ji-cuadrado con 1 grado de libertad, y que la suma de <span class="math inline">\(k\)</span> variables ji-cuadrado independientes con <span class="math inline">\(g_1, \ldots, g_k\)</span> grados de libertad sigue una distribución ji-cuadrado con <span class="math inline">\(g_1+\hdots+g_k\)</span> grados de libertad.</p>
<p>Consideremos <span class="math inline">\(n = 2\)</span>. Tenemos que <span class="math display">\[S_2^2/\sigma^2 = \frac 1 {\sigma^2}\left((X_1 - \frac {X_1 + X_2} 2)^2 +(X_2 - \frac {X_1 + X_2} 2)^2 \right)= \frac 1 {2\sigma^2} (X_1 - X_2)^2.\]</span> El término de la derecha es el cuadrado de una normal estándar, es decir una ji-cuadrado con 1 grado de libertad, lo que demuestra el caso <span class="math inline">\(n=2\)</span>.</p>
<p>Supongamos ahora que el punto 3 es cierto para <span class="math inline">\(n-1\)</span>, es decir asumimos que <span class="math inline">\((n-2)S_{n_1}/\sigma^2\sim \chi^2_{n-2}\)</span>. Vamos a usar el siguiente lema:</p>
<div id="lem-recursionSn" class="theorem lemma">
<p><span class="theorem-title"><strong>Lema 1.1 </strong></span><span id="eq-recursionSn"><span class="math display">\[\mbox{For $n\geq 2$, }\quad (n - 1)S^2_n =(n - 2)S^2_{n-1}+\frac {n-1}{n} (X_n -\bar{X}_{n -1})^2. \tag{1.2}\]</span></span></p>
</div>
<p>Para establecer <a href="#eq-recursionSn">Ecuación&nbsp;<span>1.2</span></a>, desarrollamos <span class="math inline">\(S_n^2\)</span>:</p>
<p><span class="math display">\[\begin{align*}
(n - 1)S^2_n =\sum_{i =1}^n (X_i - \bar{X}_n)^2 &amp;=&amp; \sum_{i =1}^{n-1} (X_i - \bar{X}_n)^2 +(X_n - \bar{X}_n)^2\\
&amp;=&amp; \sum_{i =1}^{n-1} (X_i - \bar{X}_{n-1}+ \bar{X}_{n-1}-\bar{X}_n)^2 +(X_n - \bar{X}_n)^2\\
&amp;=&amp; \sum_{i =1}^{n-1} (X_i - \bar{X}_{n-1})^2+ (n- 1)(\bar{X}_{n-1}-\bar{X}_n)^2 +(X_n - \bar{X}_n)^2\\
&amp;=&amp; (n - 2)S^2_{n-1}+ (n- 1)(\bar{X}_{n-1}-\bar{X}_n)^2 +(X_n - \bar{X}_n)^2\\
&amp;=&amp; (n - 2)S^2_{n-1}+ (n- 1)(\frac{\bar{X}_{n-1}-X_n} n)^2 +(\frac {n-1} n (X_n - \bar{X}_{n-1}))^2\\
&amp;=&amp; (n - 2)S^2_{n-1}+(\frac {n-1} n (X_n - \bar{X}_{n-1}))^2.\\
\end{align*}\]</span></p>
<p>Hemos usado <span class="math inline">\(\sum_{i =1}^{n-1}(X_i -\bar{X}_{n-1})=0\)</span> y <span class="math inline">\(\bar{X}_n =\frac {(n-1) \bar{X}_{n - 1}} n + \frac {X_n} n\)</span>.</p>
<p>Al ser <span class="math inline">\(X_n\)</span> y <span class="math inline">\(\bar{X}_{n-1}\)</span> variables normales independientes, deducimos que <span class="math inline">\(\frac 1 \sigma \frac {n-1} n (X_n - \bar{X}_{n-1})\)</span> es una normal estándar, por lo que su cuadrado es una ji-cuadrado con 1 grado de libertad. Es independiente de <span class="math inline">\(\bar{X}_{n-1}\)</span>, por el punto 2, y por la hipótesis de inducción, <span class="math inline">\((n - 2)S^2_{n-1}/\sigma^2\sim \chi^2_{n-2}\)</span>. Por la propiedad de sumas de variables ji-cuadrado independientes, deducimos que <span class="math inline">\((n - 1)S_n^2/ \sigma^2\sim \chi^2_{n-1}\)</span>, lo que completa la demostración por inducción del punto 3.</p>
</div>
<p>En una subsección posterior, veremos que este resultado es válido asíntoticamente (cuando <span class="math inline">\(n\)</span> tiende hacia <span class="math inline">\(+\infty\)</span>), para una clase muy grande de distribuciones <span class="math inline">\(f\)</span> de <span class="math inline">\(X\)</span>.</p>
</section>
<section id="distribución-t-de-student" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="distribución-t-de-student"><span class="header-section-number">1.6</span> Distribución t de Student</h2>
<p>Las distribuciones y los resultados obtenidos en la <a href="#prp-espvarxbar">Proposición&nbsp;<span>1.1</span></a> son esenciales cuando nuestro modelo consiste en una distribucíon normal. La <a href="#eq-xbarestandar">Ecuación&nbsp;<span>1.1</span></a> en particular será la base para construir en temas posteriores una estimación de la media “poblacional” <span class="math inline">\(\mu\)</span> que incluya un margen de error. Sin embargo, esta fórmula comprende dos parámetros a priori desconocidos, <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>. Para centrarnos en <span class="math inline">\(\mu\)</span> es natural sustituir en ella <span class="math inline">\(\sigma\)</span> por el valor de la desviación típica de la muestra <span class="math inline">\(S_n\)</span>. Es lo que hizo Gosset a principios del siglo XX, obteniendo la densidad resultante y publicando el resultado bajo el seudónimo de “Student”.</p>
<div id="prp-tstud" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposición 1.4 </strong></span>Consideramos <span class="math inline">\((X_1,\ldots,X_n)\)</span> una muestra aleatoria simple de una distribución <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>, sea <span class="math inline">\(\bar{X}\)</span> la media muestral, la distribución de los valores de <span class="math display">\[T=\frac{\bar{X}-\mu}{S/\sqrt{n}}\]</span> tiene por densidad</p>
<p><span id="eq-densstud"><span class="math display">\[ f_{t_{n -1}}(t)\propto \frac 1 {(1+t^2/(n-1))^{n/2}},\quad -\infty&lt;t&lt;\infty, \tag{1.3}\]</span></span> donde el símbolo <span class="math inline">\(\propto\)</span> significa ``es proporcional a’’, es decir que existe una constante <span class="math inline">\(K\)</span> tal que <span class="math inline">\(f_{t_{n -1}}(t) = K \frac 1 {(1+t^2/(n-1))^{n/2}},\quad -\infty&lt;t&lt;\infty\)</span>.</p>
<p>La distribución que admite esta densidad se llama distribución t de Student con <span class="math inline">\(n-1\)</span> grados de libertad. Escribimos <span class="math inline">\(T\sim t_{n-1}\)</span>.</p>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Distribución t de Student
</div>
</div>
<div class="callout-body-container callout-body">
<p>La distribución t de Student con <span class="math inline">\(k\)</span> grados de libertad está definido por su función de densidad: <span class="math display">\[ f_{t_k}(t)= \frac {\Gamma(\frac{k+1} 2)}{\Gamma(\frac k 2)}\frac 1 {\sqrt{k\pi}} \frac 1 {(1+t^2/k)^{(k+1)/2}},\quad -\infty&lt;t&lt;\infty,\]</span> donde <span class="math inline">\(\Gamma\)</span> denota la función Gamma.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Prueba</em>. </span>Vamos a demostrar que <span class="math inline">\(T=\frac{\bar{X}-\mu}{S/\sqrt{n}}\)</span> tiene por densidad la expresión dada en la <a href="#eq-densstud">Ecuación&nbsp;<span>1.3</span></a>. Para ello, introducimos las dos variables <span class="math inline">\(Z = (\bar{X}_n - \mu)/\sqrt{\sigma/n}\)</span> que sigue una distribución normal estándar y <span class="math inline">\(Y = (n-1)S_n^2/\sigma^2\)</span> que sigue una distribución ji-cuadrado con <span class="math inline">\(n-1\)</span> grados de libertad. Tenemos que <span class="math inline">\(T = Z/\sqrt{Y/(n-1)}\)</span>. Como son independientes, la densidad conjunta de <span class="math inline">\(Z\)</span> y <span class="math inline">\(Y\)</span> es el producto de sus densidades: <span class="math display">\[ f_{Z, Y}(z, y) = \frac 1 {\sqrt{2\pi}} e^{-z^2/2}\frac 1 {\Gamma(\frac{n - 1} 2) 2^{(n -  1)/2}} y^{(n - 1)/2 - 1} e^{- y/2}, \quad -\infty&lt;z&lt;\infty,\ 0&lt;y&lt;\infty.\]</span> Sea <span class="math inline">\(\Phi\)</span> la aplicación que transforma <span class="math inline">\((Z, Y)\)</span> en <span class="math inline">\((T, Y)\)</span>, <span class="math inline">\(\Phi(z, y) = (z/\sqrt{y/(n - 1)}, y)\)</span>. Se trata de un difeormorfismo de <span class="math inline">\(\mathbb{R} \times\mathbb{R}^+\)</span> a <span class="math inline">\(\mathbb{R} \times\mathbb{R}^+\)</span>. Deducimos que la densidad conjunta de <span class="math inline">\((T, Y)\)</span> es <span class="math display">\[f_{T, Y}(t, y) = f_{Z, Y}(\Phi^{-1}(t, y))\frac 1 {J\Phi(\Phi^{-1}(t, y))},\]</span> donde <span class="math inline">\(J\Phi\)</span> es el determinante Jacobiano de <span class="math inline">\(\Phi\)</span>. Tenemos que <span class="math inline">\(J\Phi(z, y) = \frac 1 {\sqrt{y/(n -1)}}\)</span>, por lo que <span class="math display">\[ f_{T, Y}(t, y) = \frac 1 {\sqrt{2\pi}} e^{-\frac {t^2 y}{2(n-1)}}\frac 1 {\Gamma(\frac{n - 1} 2) 2^{(n -  1)/2}} y^{(n - 1)/2 - 1} e^{- y/2}\sqrt{y/(n-1)}, \quad -\infty&lt;t&lt;\infty,\ 0&lt;y&lt;\infty.\]</span> Integramos respecto a <span class="math inline">\(y\)</span> para obtener la densidad marginal de <span class="math inline">\(T\)</span>: <span class="math display">\[ f_{T}(t) = \frac 1 {\sqrt{\pi(n-1)}} \frac 1 {\Gamma(\frac{n - 1} 2) 2^{n/2}}\int_0^{+\infty} y^{n/2 - 1} e^{- y/2(\frac {t^2} {n-1} + 1)}  dy.\]</span> Reconocemos en el integrando una función proporcional a la densidad de una distribución Gamma de parámetros <span class="math inline">\(\alpha = n/2\)</span> y <span class="math inline">\(\beta=2/(\frac {t^2} {n-1} + 1)\)</span>. Deducimos de la <a href="#eq-gammadens">Ecuación&nbsp;<span>1.4</span></a> que la integral del término de la derecho es igual a <span class="math inline">\(\Gamma(n/2)2^{n/2}/(\frac {t^2} {n-1} + 1)^{n/2}\)</span>.</p>
<p>En resumen, hemos obtenido que <span class="math display">\[f_{T}(t) = \frac 1 {\sqrt{\pi(n-1)}} \frac {\Gamma(n/2)} {\Gamma(\frac{n - 1} 2) }\frac 1 {(1+t^2/(n-1))^{n/2}}.\]</span></p>
</div>
<p>La distribución de <span class="math inline">\(T\)</span> depende por lo tanto del tamaño <span class="math inline">\(n\)</span> de la muestra, a través de los llamados ``grados de libertad’’.</p>
<p>La distribución t tiene colas más pesadas que la distribución Normal, lo que es intuitivamente natural puesto que, al obtenerse <span class="math inline">\(T\)</span> sustituyendo <span class="math inline">\(\sigma\)</span> por <span class="math inline">\(S\)</span>, el denominador de <span class="math inline">\(T\)</span> presenta ahora también variabilidad. Esta variabilidad en el denominador resulta en que <span class="math inline">\(T\)</span> puede tomar con más probabilidad valores más extremos. Sin embargo, si los grados de libertad aumentan, la variabilidad de <span class="math inline">\(S\)</span> disminuye, y la distribución t de Student asociada se parece más a una Normal. En la <a href="#fig-densidad-student">Figura&nbsp;<span>1.3</span></a>, están representadas la densidad de la distribución t de Student para varios valores del paramétro de grados de libertad.</p>
<div id="fig-densidad-student" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><embed src="figures/densidad_student.pdf" class="img-fluid"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;1.3: Densidad de la distribución t de Student para varios valores de los grados de libertad</figcaption><p></p>
</figure>
</div>
</section>
<section id="distribución-f-de-snedecor-para-el-cociente-de-varianzas." class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="distribución-f-de-snedecor-para-el-cociente-de-varianzas."><span class="header-section-number">1.7</span> Distribución F de Snedecor para el cociente de varianzas.</h2>
<p>Otra distribución muy importante en las aplicaciones está asociada al cociente de variables ji-cuadrado independientes y se utiliza por lo tanto para comparar varianzas muestrales correspondientes a variables diferentes o a grupos diferentes. Se llama la distribución <span class="math inline">\(F\)</span> de Snedecor y la introducimos en la siguiente definición.</p>
<div id="prp-F-Snedecor" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposición 1.5 </strong></span>Consideremos <span class="math inline">\(U_1\)</span> y <span class="math inline">\(U_2\)</span> dos variables independientes de distribución ji-cuadrado con <span class="math inline">\(p_1\)</span> y <span class="math inline">\(p_2\)</span> grados de libertad respectivamente. El cociente <span class="math inline">\(F = \frac {U_1/p_1}{U_2/p_2}\)</span> admite la densidad <span id="eq-def-F"><span class="math display">\[f_F(x) =  \frac {\Gamma(\frac {p_1 + p_2} 2)}{\Gamma(p_1)\Gamma(p_2)}(\frac{p_1}{p_2})^{p_1}\frac{x^{p_1/2-1}}{(1+ \frac {p_1}{p_2} x)^{\frac{p_1+p_2} 2}}. \tag{1.5}\]</span></span> Esta distribución se llama F de Snedecor con <span class="math inline">\(p_1\)</span> y <span class="math inline">\(p_2\)</span> grados de libertad y escribimos <span class="math inline">\(F \sim F_{p_1, p_2}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Prueba</em>. </span>Para obtener la densidad del cociente que nos interesa, procedemos como para la densidad t de Student. Escribimos la distribución conjunta de <span class="math inline">\(U_1\)</span> y <span class="math inline">\(U_2\)</span> como el producto de las distribuciones marginales, consideramos el difeomorfismo de <span class="math inline">\(\mathbb{R}^+\times\mathbb{R}^+\)</span> a <span class="math inline">\(\mathbb{R}^+ \times\mathbb{R}^+\)</span>, <span class="math inline">\((F, U_2)=\Phi(U_1, U_2) = (\frac {U_1/p_1}{U_2/p_2}, U_2)\)</span>. Su determinante Jacobiano es <span class="math inline">\(p_2/(p_1 u_2)\)</span>. Deducimos la densidad conjunta de <span class="math inline">\(F\)</span> y <span class="math inline">\(U_2\)</span> que integramos respecto a <span class="math inline">\(u_2\)</span> para obtener la densidad marginal de <span class="math inline">\(F\)</span>. Reconociendo que el integrando es proporcional a una densidad Gamma, obtenemos <a href="#eq-def-F">Ecuación&nbsp;<span>1.5</span></a>.</p>
</div>
<p>Consideremos dos variables aleatorias normales <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> con varianzas <span class="math inline">\(\sigma_2^X\)</span> y <span class="math inline">\(\sigma_2^Y\)</span>, así como <span class="math inline">\(X_1, \ldots, X_{n_1}\)</span> y <span class="math inline">\(Y_1, \ldots, Y_{n_2}\)</span> dos muestras aleatorias simples de <span class="math inline">\(X\)</span> y <span class="math inline">\(Y\)</span> respectivamente. Una consecuencia de la <a href="#prp-F-Snedecor">Proposición&nbsp;<span>1.5</span></a> es que <span class="math display">\[\frac{(n_1-1)S_{n_1}^X/\sigma_X^2}{(n_2-1)S_{n_2}^Y/\sigma_Y^2}\sim F_{n_1-1, n_2-1}.\]</span></p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>La función Gamma tiene la expresión siguiente: para cualquier real <span class="math inline">\(\alpha&gt;0\)</span>, <span class="math inline">\(\Gamma(\alpha)=\int_{0}^\infty t^{\alpha-1}e^{-t}dt.\)</span> De hecho, tiene asociada una familia de distribuciones, las llamadas distribuciones Gamma. Una distribución Gamma admite dos parámetros <span class="math inline">\(\alpha&gt;0\)</span>, y <span class="math inline">\(\beta&gt; 0\)</span> y su densidad es <span id="eq-gammadens"><span class="math display">\[ f_{\alpha, \beta}(x) = \frac 1 {\Gamma(\alpha)\beta^\alpha} x^{\alpha - 1} e^{-x/\beta}, \quad y&gt;0 \tag{1.4}\]</span></span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Prólogo</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">Referencias</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>